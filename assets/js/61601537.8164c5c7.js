"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[270],{3905:function(e,t,a){a.d(t,{Zo:function(){return d},kt:function(){return m}});var n=a(7294);function r(e,t,a){return t in e?Object.defineProperty(e,t,{value:a,enumerable:!0,configurable:!0,writable:!0}):e[t]=a,e}function i(e,t){var a=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);t&&(n=n.filter((function(t){return Object.getOwnPropertyDescriptor(e,t).enumerable}))),a.push.apply(a,n)}return a}function o(e){for(var t=1;t<arguments.length;t++){var a=null!=arguments[t]?arguments[t]:{};t%2?i(Object(a),!0).forEach((function(t){r(e,t,a[t])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(a)):i(Object(a)).forEach((function(t){Object.defineProperty(e,t,Object.getOwnPropertyDescriptor(a,t))}))}return e}function s(e,t){if(null==e)return{};var a,n,r=function(e,t){if(null==e)return{};var a,n,r={},i=Object.keys(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||(r[a]=e[a]);return r}(e,t);if(Object.getOwnPropertySymbols){var i=Object.getOwnPropertySymbols(e);for(n=0;n<i.length;n++)a=i[n],t.indexOf(a)>=0||Object.prototype.propertyIsEnumerable.call(e,a)&&(r[a]=e[a])}return r}var l=n.createContext({}),h=function(e){var t=n.useContext(l),a=t;return e&&(a="function"==typeof e?e(t):o(o({},t),e)),a},d=function(e){var t=h(e.components);return n.createElement(l.Provider,{value:t},e.children)},c={inlineCode:"code",wrapper:function(e){var t=e.children;return n.createElement(n.Fragment,{},t)}},u=n.forwardRef((function(e,t){var a=e.components,r=e.mdxType,i=e.originalType,l=e.parentName,d=s(e,["components","mdxType","originalType","parentName"]),u=h(a),m=r,p=u["".concat(l,".").concat(m)]||u[m]||c[m]||i;return a?n.createElement(p,o(o({ref:t},d),{},{components:a})):n.createElement(p,o({ref:t},d))}));function m(e,t){var a=arguments,r=t&&t.mdxType;if("string"==typeof e||r){var i=a.length,o=new Array(i);o[0]=u;var s={};for(var l in t)hasOwnProperty.call(t,l)&&(s[l]=t[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,o[1]=s;for(var h=2;h<i;h++)o[h]=a[h];return n.createElement.apply(null,o)}return n.createElement.apply(null,a)}u.displayName="MDXCreateElement"},5463:function(e,t,a){a.r(t),a.d(t,{Highlight:function(){return u},assets:function(){return d},contentTitle:function(){return l},default:function(){return p},frontMatter:function(){return s},metadata:function(){return h},toc:function(){return c}});var n=a(7462),r=a(3366),i=(a(7294),a(3905)),o=["components"],s={sidebar_position:2},l=void 0,h={unversionedId:"Data Modelling/Models",id:"Data Modelling/Models",title:"Models",description:"Starting with the simplest model, we tried to regress the meteorological parameters against",source:"@site/docs/Data Modelling/Models.md",sourceDirName:"Data Modelling",slug:"/Data Modelling/Models",permalink:"/homepage/docs/Data Modelling/Models",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Data Modelling/Models.md",tags:[],version:"current",sidebarPosition:2,frontMatter:{sidebar_position:2},sidebar:"tutorialSidebar",previous:{title:"Background",permalink:"/homepage/docs/Data Modelling/Background"},next:{title:"Tableau Dashboard",permalink:"/homepage/docs/Dashboards/Tableau Dashboard"}},d={},c=[{value:"Architecture",id:"architecture",level:2},{value:"Hyperparameter Tuning",id:"hyperparameter-tuning",level:2},{value:"Model Compilation and Callbacks",id:"model-compilation-and-callbacks",level:2},{value:"Results",id:"results",level:2}],u=function(e){var t=e.children;return(0,i.kt)("p",{style:{textAlign:"center",fontStyle:"italic"}},t)},m={toc:c,Highlight:u};function p(e){var t=e.components,s=(0,r.Z)(e,o);return(0,i.kt)("wrapper",(0,n.Z)({},m,s,{components:t,mdxType:"MDXLayout"}),(0,i.kt)("h1",{id:"linear-regression"},"Linear Regression"),(0,i.kt)("p",null,"Starting with the simplest model, we tried to regress the meteorological parameters against\nthe health parameters. We have used just a single health parameter as total malaria cases\nwhich is the total malaria cases for that region in the given month. Using regression, we tried\nto predict the dependent variable\u2019s value based on the independent variables\u2019 values using a\nline that best represents the statistical relation between the predictors and target features.\nWe also introduced a time lag (in months) in the meteorological parameters. Behind time\nlag hypothesises that the accuracy parameter will decrease as we increase the time lag and\ntry to test on the test data. The equation of linear regression is:"),(0,i.kt)(u,{mdxType:"Highlight"},"y = w0 + w1x1 + w2x2 + . . . .wnxn"),(0,i.kt)("p",null,"Linear regression models try to optimise the equation as mentioned above, that optimi\x02sation has to happen based on particular criteria called cost function."),(0,i.kt)("p",null,"Implementation of linear regression carried out using Tensorflow in python. We used\nlinear regression for malaria predictions. Normalised and generator-based data is used to\nmake predictions. Linear regression has an abysmal performance. The R squared values are\nnot greater than 2% for any CHC for the test set which is 20% of the whole time series data.\nWe aim to improve better than this baseline performance"),(0,i.kt)("h1",{id:"lstm-long-short-term-memory"},"LSTM (Long-Short Term Memory)"),(0,i.kt)("p",null,"A recurrent Neural Network(RNN) are a type of Neural Network where the current state has\ninput from the output of the previous step. If we feed a long sequence, they\u2019ll have difficulty\ncarrying information from earlier time steps to later ones. During backpropagation, recurrent\nneural networks suffer from the vanishing gradient problem. Gradients are values used to\nupdate the weights of a neural network. So in recurrent neural networks, layers that get\na minor gradient update stops learning. Those are usually the earlier layers. So because\nthese layers don\u2019t learn, RNN\u2019s can forget what is seen in longer sequences, thus having\na short-term memory. LSTM are the solution to short term memory. They have internal\nmechanisms called gates that can regulate the flow of information. A general LSTM cell has\nbeen represented in Fig. 3.1"),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Malaria_Life_Cycle",src:a(670).Z,width:"287",height:"282"})),(0,i.kt)(u,{mdxType:"Highlight"},"Figure 3.1: Representation of LSTM cell. Source: On the Origin of Deep Learning[33]"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("strong",{parentName:"li"},"input gate")," and ",(0,i.kt)("strong",{parentName:"li"},"forget gate")," determine how the next internal state is influenced\nby the input, and the last internal state respectively."),(0,i.kt)("li",{parentName:"ul"},"The ",(0,i.kt)("strong",{parentName:"li"},"output gate")," determines how the output of the network is influenced by the\ninternal state. Sometimes, it\u2019s helpful to remember important details but only use\nthem later.")),(0,i.kt)("h2",{id:"architecture"},"Architecture"),(0,i.kt)("p",null,"This is a vanilla LSTM model with just 2 sequential layers. First layer is a LSTM layer\nwith 32 nodes and has a input shape of number of variables and window length of 4. ELU\n( Exponential Linear Unit) used activation function. The second layer is a dense layer with\none unit."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Activation function - ELU :")," Exponential Linear Unit(ELU) is an alternative to\nReLU activation function. It tend to converge cost to zero faster and produce more ac\x02curate results. ELU approaches smoothness slowly until its output reaches a parameter\n\u03b1 ulike ReLU which sharply smoothens."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Optimiser - Adam:")," It is combination of two gradient descend optimisation tech\x02niques: momentum and Root mean square propagation (RSMP). Momentum accel\x02erates the learning algorithm while RSMP is a adaptive learning algorithm. Adam\nhas 3 main hyperparameters: decay rate of the two methods (\u03b21&\u03b22) and a step size\nparameter or learning rate \u03b1"),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Loss Function - MSE:")," The mean squared error(MSE) tells us how close a regression\nline is to a set of points. It does this by taking the squared distances from the points\nto the regression line. The squaring removes any negative signs. The smaller the mean\nsquared error, the better the fit or model."),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Metrics - MAE:")," The mean absolute error(MAE) measures the average magnitude\nof the errors in a set of forecasts, without considering their direction. It measures\naccuracy for continuous variables")),(0,i.kt)("h2",{id:"hyperparameter-tuning"},"Hyperparameter Tuning"),(0,i.kt)("p",null,"As we have discussed, in the LSTM network, few gates and nodes define the flow of data.\nAs the data flows, we calculate the derivatives and cost functions, and at the end of the\ntraining, we get the associated coefficients which we roughly call the model parameters. So\nduring model training, only parameters are calculated and updated, but a few parameters are\nnot directly derived from gradients but directly affect the model. These \u2019extra\u2019 parameters\nusually defined before the learning begins are called hyper-parameters. Finding a perfect\nhyperparameter for the specific model and data set is very difficult as few of these are very\nsensitive to initial conditions, and thus a tiny change in their values may lead to a very\ndifferent model learning."),(0,i.kt)("p",null,"In our LSTM model, we have adam hyperparameters, learning rate, number of units\nin LSTM, different activation functions, number of epochs, and batch size as our hyper\x02parameter space which we intended to tweak and find the best optimal value. We used\nthe hyperparameter optimisation technique called grid search. In this algorithm, we de\x02fine a hyperparameter space for each of the hyperparameters, and then this algorithm uses\npermutations to try out models on each combination."),(0,i.kt)("p",null,"The implementation of this grid search is done via Keras grid search. The grid search\ncomputes the metrics defined at each combination, and at the end of grid-search, we get\nthe best model. The choice of model is manual. We took the values and metrics for each\ncombination in an excel file and manually found the model with optimal performance on\ntraining and the validation set. Keras grid search can automatically output the best model,\nbut this generally considers the better performance on the validation set, which sometimes\nleads to over or under-fitting."),(0,i.kt)("h2",{id:"model-compilation-and-callbacks"},"Model Compilation and Callbacks"),(0,i.kt)("p",null,"After the model compilation using the hyperparameters obtained via grid search with loss\nand metrics, we have moved to model training which we will discuss in the next section.\nBefore model training, we have made some callbacks that get triggered only when certain\nconditions are met."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Early Stopping:")," This callback can stop the model training at a desired loss or\nmetrics value. In our implementation, if the validation loss value does not change over\nfive epochs, then model training will stop. This method can save us time and extra\ncomputing power.")),(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("p",{parentName:"li"},(0,i.kt)("strong",{parentName:"p"},"Model Checkpoint:")," This callback saves the model weight at the regular time interval.\nThe model weights can then be used to forecast the cases where live training is not\nideal, like our dashboard(discussed later)"))),(0,i.kt)("h2",{id:"results"},"Results"),(0,i.kt)("p",null,"The performance of the vanilla LSTM model is better than the Linear Regression but not\noutstanding. We have improved very little, and the R-squared value is just above 5. There\nare many reasons for this performance."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},"We are training on individual time series and making individual predictions. Since we\nhave monthly average data, the number of data points is much less. Out of these, we\nhave taken out 20% of 70-80 data points for testing, so having fewer data points will\nseverely impact our model performance."),(0,i.kt)("li",{parentName:"ul"},"Another reason might be the hyper meter space due to the lack of data points to check.\nAlso, upon the Fourier analysis of the data, many peak frequencies indicate much noise\nin the data and if we try to take only a few top frequencies and do an inverse Fourier\ntransform to generate data with less noise. This noiseless data will perform better in\ntraining but introduce a bias in the small data, which again implies a more significant\nvariance in the test data")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Malaria_Life_Cycle",src:a(9207).Z,width:"363",height:"235"})),(0,i.kt)(u,{mdxType:"Highlight"},"Figure 3.2: Epochs vs Error plot of LSTM model at lead time 1 month"),(0,i.kt)("p",null,"Since the data points are very few, an obvious solution would be to get more data points,\nbut that\u2019s a very different task and is subjected to many factors. With the limited data\nscope, we can use Bayesian hyperparameter tuning with the Fourier transformed data to get\na better set of individual data sets. Another solution would be to use an ensemble model like\nDeepAR, which doesn\u2019t differentiate between time series and makes a unified model for each\ntime series. This type of algorithm will solve our data scarcity problem. In the following\nsections, we will work towards building the DeepAR models"))}p.isMDXComponent=!0},9207:function(e,t,a){t.Z=a.p+"assets/images/error-56b8ce3a5e2eff394c9428ccab51870f.png"},670:function(e,t,a){t.Z=a.p+"assets/images/lstm-e90a45b2a1359769847f123cc23f5fcd.jpg"}}]);