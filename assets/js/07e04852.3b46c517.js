"use strict";(self.webpackChunkwebsite=self.webpackChunkwebsite||[]).push([[90],{3905:function(e,a,t){t.d(a,{Zo:function(){return c},kt:function(){return u}});var n=t(7294);function r(e,a,t){return a in e?Object.defineProperty(e,a,{value:t,enumerable:!0,configurable:!0,writable:!0}):e[a]=t,e}function o(e,a){var t=Object.keys(e);if(Object.getOwnPropertySymbols){var n=Object.getOwnPropertySymbols(e);a&&(n=n.filter((function(a){return Object.getOwnPropertyDescriptor(e,a).enumerable}))),t.push.apply(t,n)}return t}function i(e){for(var a=1;a<arguments.length;a++){var t=null!=arguments[a]?arguments[a]:{};a%2?o(Object(t),!0).forEach((function(a){r(e,a,t[a])})):Object.getOwnPropertyDescriptors?Object.defineProperties(e,Object.getOwnPropertyDescriptors(t)):o(Object(t)).forEach((function(a){Object.defineProperty(e,a,Object.getOwnPropertyDescriptor(t,a))}))}return e}function s(e,a){if(null==e)return{};var t,n,r=function(e,a){if(null==e)return{};var t,n,r={},o=Object.keys(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||(r[t]=e[t]);return r}(e,a);if(Object.getOwnPropertySymbols){var o=Object.getOwnPropertySymbols(e);for(n=0;n<o.length;n++)t=o[n],a.indexOf(t)>=0||Object.prototype.propertyIsEnumerable.call(e,t)&&(r[t]=e[t])}return r}var l=n.createContext({}),d=function(e){var a=n.useContext(l),t=a;return e&&(t="function"==typeof e?e(a):i(i({},a),e)),t},c=function(e){var a=d(e.components);return n.createElement(l.Provider,{value:a},e.children)},h={inlineCode:"code",wrapper:function(e){var a=e.children;return n.createElement(n.Fragment,{},a)}},p=n.forwardRef((function(e,a){var t=e.components,r=e.mdxType,o=e.originalType,l=e.parentName,c=s(e,["components","mdxType","originalType","parentName"]),p=d(t),u=r,m=p["".concat(l,".").concat(u)]||p[u]||h[u]||o;return t?n.createElement(m,i(i({ref:a},c),{},{components:t})):n.createElement(m,i({ref:a},c))}));function u(e,a){var t=arguments,r=a&&a.mdxType;if("string"==typeof e||r){var o=t.length,i=new Array(o);i[0]=p;var s={};for(var l in a)hasOwnProperty.call(a,l)&&(s[l]=a[l]);s.originalType=e,s.mdxType="string"==typeof e?e:r,i[1]=s;for(var d=2;d<o;d++)i[d]=t[d];return n.createElement.apply(null,i)}return n.createElement.apply(null,t)}p.displayName="MDXCreateElement"},5050:function(e,a,t){t.r(a),t.d(a,{assets:function(){return h},contentTitle:function(){return d},default:function(){return f},frontMatter:function(){return l},metadata:function(){return c},toc:function(){return p}});var n,r=t(7462),o=t(3366),i=(t(7294),t(3905)),s=["components"],l={sidebar_position:3},d=void 0,c={unversionedId:"Data Management/Data Preprocessing and Database Management",id:"Data Management/Data Preprocessing and Database Management",title:"Data Preprocessing and Database Management",description:"Data pre-processing is crucial in defining the data for the model development and visualiza\x02tions. The health data we get from the field has many missing values, and some data is not",source:"@site/docs/Data Management/Data Preprocessing and Database Management.md",sourceDirName:"Data Management",slug:"/Data Management/Data Preprocessing and Database Management",permalink:"/homepage/docs/Data Management/Data Preprocessing and Database Management",editUrl:"https://github.com/facebook/docusaurus/tree/main/packages/create-docusaurus/templates/shared/docs/Data Management/Data Preprocessing and Database Management.md",tags:[],version:"current",sidebarPosition:3,frontMatter:{sidebar_position:3},sidebar:"tutorialSidebar",previous:{title:"Data Sources",permalink:"/homepage/docs/Data Management/Data Sources"},next:{title:"Background",permalink:"/homepage/docs/Data Modelling/Background"}},h={},p=[{value:"Health Data Cleaning and Preprocessing",id:"health-data-cleaning-and-preprocessing",level:2},{value:"Database Management",id:"database-management",level:2}],u=(n="Highlight",function(e){return console.warn("Component "+n+" was not imported, exported, or provided by MDXProvider as global scope"),(0,i.kt)("div",e)}),m={toc:p};function f(e){var a=e.components,n=(0,o.Z)(e,s);return(0,i.kt)("wrapper",(0,r.Z)({},m,n,{components:a,mdxType:"MDXLayout"}),(0,i.kt)("p",null,"Data pre-processing is crucial in defining the data for the model development and visualiza\x02tions. The health data we get from the field has many missing values, and some data is not\nin the correct format. We need to clean the data and prepare it for the model development.\nIn climate data, the date generally comes in the gridded or binary format, which has to\nbe converted to a tabular format and then appended to an existing database. The data\nis already in a tabular form in health data, and we can directly append it to the current\ndatabase once cleaned and processed."),(0,i.kt)("h2",{id:"health-data-cleaning-and-preprocessing"},"Health Data Cleaning and Preprocessing"),(0,i.kt)("p",null,"Health data comes in an excel file with multiple sheets. We need to find the sheets with\nrelevant data for each task. Currently, we have two pipelines for the data cleaning and\nformatting process, python based pre-processing for model development and Alteryx-based\npre-processing for metadata management and hard-to-process data. Alteryx is an industry\x02leading data no-code platform with drag and drop features to make single switch pipelines.\nWe will take individual leads on each method-"),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Python-based open source pre-processing:")," This pre-processing method is currently in\ndevelopment. We will start with a data storage facility hosted on Azure Blob Storage.\nThe data comes directly from the field via the data portal. There is a manual task\nto assign a sheet name to process each time. In the future, we will incorporate a\nweb-based interface to ask for the sheet name. As the data is uploaded to the upload\nportal, it gets renamed, and a date time index gets attached to the file name to identify\nthe data and sort by time to find the latest file. We will also put the date and month\nindex in the web-based interface as a user input by the data uploader agent on the\nportal. Now, after the data gets uploaded and the sheet name is identified, which\ncontains the SC or CHC-based data, we run a python based custom pipeline in the\nAzure data factory. Azure data factory has links to Blob Storage via a security key.\nOur data warehouse, Azure SQL Database, has also been linked via the different pipelines in\nthe Azure Data Factory. Once the data gets pre-processed via python script, it gets\nsaved in a separate container in the Blob Storage. After the manual review of the pre-\nprocessing quality, a new pipeline is manually triggered to upload the output data to\nthe Azure SQL Database. We aim to fully automate this process of uploading data via the portal\nand then pre-processing to uploading it to a Azure SQL Database table. The output folder for a\nmanual review is a redundant process, and its removal is future development. There\nare several break point in the system which has to be repaired. The data that comes\nup is an excel format with merged cells as column headers, and our python script has\na fixed recognization pattern, so if the pattern of cell merger changes a little, then the\nexception commands trigger. The system will bypass the data file. At this point, we\ninclude more checkpoints in the system to counter the exception raised, and then the\nAlteryx-based pre-processing kicks in. Still, it requires files to be downloaded to the\nlocal machine, and then the processed file can be directly uploaded to the Azure SQL Database\nvia a custom python connector. In pre-processing, we do the following.")),(0,i.kt)("ol",null,(0,i.kt)("li",{parentName:"ol"},"Find the sheets which have relevant data for each task."),(0,i.kt)("li",{parentName:"ol"},"Remove the header column with merged cells"),(0,i.kt)("li",{parentName:"ol"},"Replace the header with our defined header (the columns should be in proper\norder)"),(0,i.kt)("li",{parentName:"ol"},"Crop the rows and columns which has data (redundant rows and columns are\ngenerally present)"),(0,i.kt)("li",{parentName:"ol"},"Look for missing data"),(0,i.kt)("li",{parentName:"ol"},"Check it every column satisfies its data type"),(0,i.kt)("li",{parentName:"ol"},"If there is a data type error, then raise an error and bypass the system"),(0,i.kt)("li",{parentName:"ol"},"Check for the name of CHC and SC and replace them with the correct name via\nfuzzy logic matching"),(0,i.kt)("li",{parentName:"ol"},"Send the processed file to an output container in the Blob Storage"),(0,i.kt)("li",{parentName:"ol"},"After reviewing the data in the output container, upload the data to the Azure SQL Database\ntable")),(0,i.kt)("p",null,"We have also started to implement Spark based data processing of data coming through upload portal and then pushing it to Azure SQl data base. Python and SQL-based DataBricks tool are two popular methods for processing data, each with its own strengths and use cases. Python provides a variety of powerful libraries for data manipulation, such as pandas and numpy, which enable users to clean and transform data, perform calculations, and extract insights. Python also has extensive support for machine learning and artificial intelligence algorithms, making it a popular choice for data science applications. SQL-based DataBricks tool is a cloud-based platform that provides a unified analytics engine for big data processing. DataBricks is built on top of Apache Spark, an open-source distributed computing system that enables fast and scalable processing of large data sets. DataBricks provides a collaborative environment for data scientists, analysts, and other users to work together and share insights. It supports a range of big data analytics use cases, including data warehousing, machine learning, and streaming analytics. When it comes to data processing, Python is generally preferred for smaller datasets that can be processed on a single machine. SQL-based DataBricks tool is better suited for larger datasets that require distributed processing across multiple machines. However, the choice between these two methods depends on the specific needs of the project, the size and complexity of the dataset, and the skills and preferences of the data processing team."),(0,i.kt)("p",null,"In summary, both Python and SQL-based DataBricks tool are powerful tools for data processing, with their own strengths and use cases."),(0,i.kt)("ul",null,(0,i.kt)("li",{parentName:"ul"},(0,i.kt)("strong",{parentName:"li"},"Alteryx-based pre-processing")," - This method of development is very robust. We have\nused every proofing technique to get the true and desired data table. Since Alteryx is a\npaid software and connectors are not available for Azure so we can\u2019t use it to automate\nour process. We have used Alteryx to create a pipeline that can be triggered manually.\nWe have used joins, filters, and even python scripts to get the desired data table. We\nhave used a custom python connector to upload the data to the Azure SQL Database table. We\nfollow the same pre-processing steps as we have defined above.\nFig 2.1 shows a sample workflow for the Koraput district pre-processing. There are\nfour blocks(top to bottom). The top block imports the data from the data lake, and\nthe following block helps view the data. The next block does the data engineering\ntask. Finally, the last block governs the name entity correction, and at the end, it\noutputs a file which has to be manually uploaded to the data lake and then pushed to\nthe database.")),(0,i.kt)("p",null,(0,i.kt)("img",{alt:"Malaria_Life_Cycle",src:t(8811).Z,width:"379",height:"739"})),(0,i.kt)(u,{mdxType:"Highlight"},"Figure 2.5: Sample Alteryx workflow for the Koraput District data pre-processing. This workflow can fix missing data, name entity correction, shape formatting, etc."),(0,i.kt)("h2",{id:"database-management"},"Database Management"),(0,i.kt)("p",null,"Azure SQL Database is a cloud-based relational database management system (RDBMS) that is part of Microsoft's Azure cloud computing platform. It is designed to provide a scalable, secure, and fully-managed database service for applications running in the cloud. Azure SQL Database is built on the same engine as Microsoft SQL Server, which is a popular RDBMS used by many organizations. It provides many of the same features and capabilities as SQL Server, such as support for relational data, advanced querying capabilities, and built-in security features."),(0,i.kt)("p",null,"One of the key benefits of Azure SQL Database is its scalability. It can be easily scaled up or down based on the needs of the application, allowing organizations to pay only for the resources they need. It also provides high availability and disaster recovery features, such as automatic backups and geo-replication, which can help ensure that data is always available and protected Azure SQL Database is also highly secure. It includes built-in security features, such as dynamic data masking and row-level security, which can help protect sensitive data. It also integrates with Azure Active Directory, which provides centralized user management and authentication."),(0,i.kt)("p",null,"Overall, Azure SQL Database is a powerful and flexible cloud-based RDBMS that can help organizations to efficiently manage and scale their data in the cloud. Its scalability, security features, and integration with other Azure services make it a popular choice for cloud-based applications."),(0,i.kt)("p",null,"We have Odisha as a schema, and each district has its table. There is also a different\ntable for metadata management. In Azure SQL Database, there is also a feature of simple dashboarding\nfor simple visualization to get a feel of the data. Azure SQL Database has multiple connectors for\ndifferent data platforms like Alteryx, Tableau, and Azure. Tableau is an industry-standard\ndashboarding tool. We have used Tableau to visualize the data, which will be discussed in\ndetail in the Dashboard chapter"))}f.isMDXComponent=!0},8811:function(e,a,t){a.Z=t.p+"assets/images/alteryx-3185a8a600ac80873eed4052582820a8.jpg"}}]);